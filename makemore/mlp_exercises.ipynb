{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import random\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defaults\n",
    "DIR_READ = pathlib.Path(__name__).resolve().parent\n",
    "DIR_OUT = pathlib.Path(__name__).resolve().parents[1] / \"out\" / \"makemore-mlp\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "g = torch.Generator(device=device).manual_seed(2147483647) # default seed from the torch docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data to tokens\n",
    "def load_txt_to_list(fname):\n",
    "    path = pathlib.Path(fname)\n",
    "    if not path.exists():\n",
    "        raise f\"{fname} not found!\"\n",
    "\n",
    "    list_out = []\n",
    "    with open(path, \"r\") as file:\n",
    "        list_out = file.read().splitlines()\n",
    "\n",
    "    return list_out\n",
    "\n",
    "def construct_map_token_to_index(list_tokens):\n",
    "    \"\"\"\n",
    "    Returns { token : index }\n",
    "\n",
    "    Map each token in the vocab to a unique integer,\n",
    "    which will be its index into the Bag of words vector\n",
    "\n",
    "    TODO: https://en.wikipedia.org/wiki/Feature_hashing\n",
    "    NOTE: Fatal flaw is set sorting is random, making debugging a little harder\n",
    "    \"\"\"\n",
    "    dict_to_ix = {}\n",
    "    dict_to_word = {}\n",
    "    for i, token in enumerate(set(list_tokens)):\n",
    "        dict_to_ix[token] = i\n",
    "        dict_to_word[i] = token\n",
    "\n",
    "    return dict_to_ix, dict_to_word\n",
    "\n",
    "fname = DIR_READ / \"names.txt\"\n",
    "words = load_txt_to_list(fname)\n",
    "\n",
    "dict_token_to_ix, dict_ix_to_token = construct_map_token_to_index(\"\".join(words))\n",
    "\n",
    "list_tokens_extra = [\".\"]\n",
    "for token in list_tokens_extra:\n",
    "    dict_token_to_ix[token] = len(dict_token_to_ix)\n",
    "    dict_ix_to_token[len(dict_ix_to_token)] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '.', '.', 'e'] [26, 26, 26, 6]\n",
      "['.', '.', 'e', 'm'] [26, 26, 6, 21]\n",
      "['.', 'e', 'm', 'm'] [26, 6, 21, 21]\n",
      "['e', 'm', 'm', 'a'] [6, 21, 21, 8]\n",
      "['m', 'm', 'a', '.'] [21, 21, 8, 26]\n",
      "['.', '.', '.', 'o'] [26, 26, 26, 6]\n",
      "['.', '.', 'o', 'l'] [26, 26, 6, 21]\n",
      "['.', 'o', 'l', 'i'] [26, 6, 21, 21]\n",
      "['o', 'l', 'i', 'v'] [6, 21, 21, 8]\n",
      "['l', 'i', 'v', 'i'] [21, 21, 8, 26]\n",
      "['i', 'v', 'i', 'a'] [21, 8, 26, 23]\n",
      "['v', 'i', 'a', '.'] [8, 26, 23, 15]\n",
      "['.', '.', '.', 'a'] [26, 26, 26, 6]\n",
      "['.', '.', 'a', 'v'] [26, 26, 6, 21]\n",
      "['.', 'a', 'v', 'a'] [26, 6, 21, 21]\n",
      "['a', 'v', 'a', '.'] [6, 21, 21, 8]\n"
     ]
    }
   ],
   "source": [
    "# Calculating the ngrams\n",
    "list_documents = [ list(string) + [\".\"] for string in words ] # different from before\n",
    "\n",
    "def construct_vector_ngram(tokens_context, dict_index):\n",
    "    return list(map(lambda w: dict_index[w], tokens_context))\n",
    "\n",
    "def construct_n_grams(list_documents, dict_index, size_context, size_prediction=1):\n",
    "    \"\"\"\n",
    "    Initialize an empty window\n",
    "    Update as you go along\n",
    "\n",
    "    NOTE: Creates size_context-1 more vectors to account for the stop tokens\n",
    "    e.g., size_context = 3-1 creates\n",
    "        [<S>, <S>, <T>]\n",
    "\n",
    "    TODO: Account for when size_prediction is larger than length(document)\n",
    "    \"\"\"\n",
    "    list_out = []\n",
    "    vector = [dict_index[\".\"]]*size_context\n",
    "    for tokens in list_documents:\n",
    "        # TODO: Right pad tokens to size_prediction\n",
    "        for i in range(0, len(tokens)):\n",
    "            vector = vector[size_prediction:] + construct_vector_ngram(tokens[i:i+size_prediction], dict_index)\n",
    "            list_out.append(vector)\n",
    "    return list_out\n",
    "\n",
    "SIZE_CONTEXT = 4 # SIZE_NGRAMS+1 or BLOCKSIZE\n",
    "list_vectors = construct_n_grams(list_documents, dict_token_to_ix, size_context=SIZE_CONTEXT)\n",
    "# Verify the mapping\n",
    "for document in list_documents[:3]:\n",
    "    for i in range(len(document)): # num_ngrams\n",
    "        print(([\".\"]*(SIZE_CONTEXT-1) + document)[i:i+SIZE_CONTEXT], list_vectors[i])\n",
    "\n",
    "matrix_ngrams = torch.tensor(list_vectors, dtype = torch.int64, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E01: Hyperparameter Search\n",
    "Tune the hyperparameters of the training to beat my best validation loss of 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author's Note: I KNOW this can be done easier in pytorch & sklearn, but it's better to see how this works for educational reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, Y, list_parameters):\n",
    "    \"\"\"\n",
    "    logits = tanh \\circ (C[X] @ W1 + b1)) @ W2 + b2\n",
    "    loss = cross_entropy(logits, Y)\n",
    "    where \n",
    "        X is the tokenized inputs, Y is the tokenized outputs\n",
    "        C is the (|V|, k) mapping that embeds X into a lower dimensional space\n",
    "        W1 is the (k, hidden) weight matrix\n",
    "        b1 is the (hidden, 1) bias vector\n",
    "        W2 is the (hidden, |V|) weight matrix\n",
    "        b2 = is the (|V|) bias vector\n",
    "    \n",
    "    i.e,\n",
    "        logits = linear \\circ tanh \\circ linear \\circ embedding\n",
    "    \"\"\"\n",
    "    embed = list_parameters[0][X] # shape (num_ngrams[indices], SIZE_CONTEXT-1, SIZE_DIMENSION)\n",
    "    h = torch.tanh(torch.einsum('ijk,jkl -> il', embed, list_parameters[1]) + list_parameters[2]) # hidden states\n",
    "    logits = h @ list_parameters[3] + list_parameters[4]\n",
    "    loss = torch.nn.functional.cross_entropy(logits, Y)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def train_neural_net(xtr, ytr, list_parameters, num_epochs = 30000, size_batch = 35, tag = None):\n",
    "    list_steps = []\n",
    "    list_loss = []\n",
    "\n",
    "    LEARNING_RATE = 0.1 # discovered empirocally\n",
    "    epoch_decay = num_epochs*0.9 # after 90% of epochs\n",
    "    for p in list_parameters:\n",
    "        p.requires_grad=True\n",
    "    for i in range(num_epochs):\n",
    "        # mini-batch\n",
    "        indices = torch.randint(0, xtr.shape[0], (size_batch,))\n",
    "        loss = forward_pass(xtr[indices], ytr[indices], list_parameters)\n",
    "        # backward pass\n",
    "        for p in list_parameters:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        if i == epoch_decay:\n",
    "            print(\"\\tLoss before learning rate decay:\", loss.item())\n",
    "            LEARNING_RATE = LEARNING_RATE * 0.1\n",
    "        for p in list_parameters:\n",
    "            p.data += -LEARNING_RATE * p.grad # going against the gradient reduces the loss\n",
    "        list_steps.append(i)\n",
    "        list_loss.append(loss.log10().item())\n",
    "\n",
    "    print(\"\\tLoss after learning rate decay:\", loss.item())\n",
    "    plt.title(f\"Train log10-loss {tag}.png\")\n",
    "    plt.plot(list_steps, list_loss)\n",
    "    plt.savefig(DIR_OUT / f\"train-log10-loss-{sum(p.numel() for p in list_parameters)}-{tag}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    return list_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "random.seed(42)\n",
    "random.shuffle(matrix_ngrams.clone())\n",
    "xs = matrix_ngrams[:,0:SIZE_CONTEXT-1]\n",
    "ys = matrix_ngrams[:,-1] # vector\n",
    "\n",
    "n1 = int(0.8 * xs.shape[0])\n",
    "n2 = int(0.9 * xs.shape[0])\n",
    "Xtr, Xdev, Xts = xs.tensor_split((n1, n2), dim=0)\n",
    "Ytr, Ydev, Yts = ys.tensor_split((n1, n2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPERPARAMETER-TRAINING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING NEURAL NET with (DIMENSION=4, HIDDEN=100)\n",
      "\tLoss before learning rate decay: 2.3964381217956543\n",
      "\tLoss after learning rate decay: 2.3666436672210693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:49<19:48, 49.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.322718620300293\n",
      "\tValidate loss: 2.542935371398926\n",
      "\tBest loss updated for (DIMENSION, HIDDEN): (4, 100)\n",
      "TRAINING NEURAL NET with (DIMENSION=4, HIDDEN=200)\n",
      "\tLoss before learning rate decay: 2.39103102684021\n",
      "\tLoss after learning rate decay: 2.2972142696380615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [01:30<17:03, 44.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.3347537517547607\n",
      "\tValidate loss: 2.5405962467193604\n",
      "\tBest loss updated for (DIMENSION, HIDDEN): (4, 200)\n",
      "TRAINING NEURAL NET with (DIMENSION=4, HIDDEN=300)\n",
      "\tLoss before learning rate decay: 2.336271047592163\n",
      "\tLoss after learning rate decay: 2.1987054347991943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/25 [02:13<16:00, 43.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.334138870239258\n",
      "\tValidate loss: 2.568892478942871\n",
      "TRAINING NEURAL NET with (DIMENSION=4, HIDDEN=400)\n",
      "\tLoss before learning rate decay: 2.5457892417907715\n",
      "\tLoss after learning rate decay: 2.214876413345337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4/25 [03:00<15:44, 44.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.3009350299835205\n",
      "\tValidate loss: 2.5354981422424316\n",
      "\tBest loss updated for (DIMENSION, HIDDEN): (4, 400)\n",
      "TRAINING NEURAL NET with (DIMENSION=4, HIDDEN=500)\n",
      "\tLoss before learning rate decay: 2.8082199096679688\n",
      "\tLoss after learning rate decay: 2.416879177093506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [03:54<16:04, 48.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.334836959838867\n",
      "\tValidate loss: 2.5623323917388916\n",
      "TRAINING NEURAL NET with (DIMENSION=6, HIDDEN=100)\n",
      "\tLoss before learning rate decay: 2.4254281520843506\n",
      "\tLoss after learning rate decay: 2.33101224899292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [04:36<14:39, 46.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.3125319480895996\n",
      "\tValidate loss: 2.532827854156494\n",
      "\tBest loss updated for (DIMENSION, HIDDEN): (6, 100)\n",
      "TRAINING NEURAL NET with (DIMENSION=6, HIDDEN=200)\n",
      "\tLoss before learning rate decay: 2.2421605587005615\n",
      "\tLoss after learning rate decay: 2.231884002685547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [05:29<14:32, 48.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.316272497177124\n",
      "\tValidate loss: 2.5506680011749268\n",
      "TRAINING NEURAL NET with (DIMENSION=6, HIDDEN=300)\n",
      "\tLoss before learning rate decay: 2.4359896183013916\n",
      "\tLoss after learning rate decay: 2.237220525741577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8/25 [07:31<20:20, 71.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.298750638961792\n",
      "\tValidate loss: 2.5589308738708496\n",
      "TRAINING NEURAL NET with (DIMENSION=6, HIDDEN=400)\n",
      "\tLoss before learning rate decay: 2.676542043685913\n",
      "\tLoss after learning rate decay: 2.2643282413482666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9/25 [09:15<21:47, 81.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.305476427078247\n",
      "\tValidate loss: 2.57936429977417\n",
      "TRAINING NEURAL NET with (DIMENSION=6, HIDDEN=500)\n",
      "\tLoss before learning rate decay: 2.7118258476257324\n",
      "\tLoss after learning rate decay: 2.4553041458129883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10/25 [10:29<19:53, 79.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.293982982635498\n",
      "\tValidate loss: 2.5545291900634766\n",
      "TRAINING NEURAL NET with (DIMENSION=8, HIDDEN=100)\n",
      "\tLoss before learning rate decay: 2.175222873687744\n",
      "\tLoss after learning rate decay: 2.277735948562622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [11:10<15:45, 67.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.2877840995788574\n",
      "\tValidate loss: 2.517744541168213\n",
      "\tBest loss updated for (DIMENSION, HIDDEN): (8, 100)\n",
      "TRAINING NEURAL NET with (DIMENSION=8, HIDDEN=200)\n",
      "\tLoss before learning rate decay: 2.3705813884735107\n",
      "\tLoss after learning rate decay: 2.390019178390503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12/25 [12:37<15:58, 73.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.295909881591797\n",
      "\tValidate loss: 2.5582034587860107\n",
      "TRAINING NEURAL NET with (DIMENSION=8, HIDDEN=300)\n",
      "\tLoss before learning rate decay: 2.272771120071411\n",
      "\tLoss after learning rate decay: 2.2247745990753174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [13:48<14:32, 72.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.284935474395752\n",
      "\tValidate loss: 2.550629138946533\n",
      "TRAINING NEURAL NET with (DIMENSION=8, HIDDEN=400)\n",
      "\tLoss before learning rate decay: 2.427489757537842\n",
      "\tLoss after learning rate decay: 2.2385025024414062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14/25 [14:36<11:59, 65.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.2648022174835205\n",
      "\tValidate loss: 2.5050528049468994\n",
      "\tBest loss updated for (DIMENSION, HIDDEN): (8, 400)\n",
      "TRAINING NEURAL NET with (DIMENSION=8, HIDDEN=500)\n",
      "\tLoss before learning rate decay: 2.4847939014434814\n",
      "\tLoss after learning rate decay: 2.235915184020996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15/25 [15:29<10:15, 61.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.341719627380371\n",
      "\tValidate loss: 2.6820766925811768\n",
      "TRAINING NEURAL NET with (DIMENSION=10, HIDDEN=100)\n",
      "\tLoss before learning rate decay: 2.2224602699279785\n",
      "\tLoss after learning rate decay: 2.0046744346618652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16/25 [16:55<10:20, 68.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.2699227333068848\n",
      "\tValidate loss: 2.5082297325134277\n",
      "TRAINING NEURAL NET with (DIMENSION=10, HIDDEN=200)\n",
      "\tLoss before learning rate decay: 2.300025463104248\n",
      "\tLoss after learning rate decay: 2.40895938873291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17/25 [18:10<09:25, 70.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.2644553184509277\n",
      "\tValidate loss: 2.521078109741211\n",
      "TRAINING NEURAL NET with (DIMENSION=10, HIDDEN=300)\n",
      "\tLoss before learning rate decay: 2.5672144889831543\n",
      "\tLoss after learning rate decay: 2.337656021118164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 18/25 [19:13<07:58, 68.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.300452947616577\n",
      "\tValidate loss: 2.6176979541778564\n",
      "TRAINING NEURAL NET with (DIMENSION=10, HIDDEN=400)\n",
      "\tLoss before learning rate decay: 2.3359336853027344\n",
      "\tLoss after learning rate decay: 2.273834705352783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 19/25 [20:18<06:45, 67.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.288207530975342\n",
      "\tValidate loss: 2.6203348636627197\n",
      "TRAINING NEURAL NET with (DIMENSION=10, HIDDEN=500)\n",
      "\tLoss before learning rate decay: 2.628025770187378\n",
      "\tLoss after learning rate decay: 2.117724895477295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20/25 [21:43<06:03, 72.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.2509899139404297\n",
      "\tValidate loss: 2.5061728954315186\n",
      "TRAINING NEURAL NET with (DIMENSION=12, HIDDEN=100)\n",
      "\tLoss before learning rate decay: 2.37005352973938\n",
      "\tLoss after learning rate decay: 2.2300894260406494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21/25 [23:02<04:58, 74.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.2816789150238037\n",
      "\tValidate loss: 2.5426034927368164\n",
      "TRAINING NEURAL NET with (DIMENSION=12, HIDDEN=200)\n",
      "\tLoss before learning rate decay: 2.4014499187469482\n",
      "\tLoss after learning rate decay: 2.1604673862457275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22/25 [24:23<03:49, 76.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.2337403297424316\n",
      "\tValidate loss: 2.4923994541168213\n",
      "\tBest loss updated for (DIMENSION, HIDDEN): (12, 200)\n",
      "TRAINING NEURAL NET with (DIMENSION=12, HIDDEN=300)\n",
      "\tLoss before learning rate decay: 2.4785332679748535\n",
      "\tLoss after learning rate decay: 2.1955628395080566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 23/25 [25:56<02:43, 81.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.2575223445892334\n",
      "\tValidate loss: 2.587883234024048\n",
      "TRAINING NEURAL NET with (DIMENSION=12, HIDDEN=400)\n",
      "\tLoss before learning rate decay: 2.448808431625366\n",
      "\tLoss after learning rate decay: 2.54472017288208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24/25 [27:11<01:19, 79.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.2624053955078125\n",
      "\tValidate loss: 2.6233489513397217\n",
      "TRAINING NEURAL NET with (DIMENSION=12, HIDDEN=500)\n",
      "\tLoss before learning rate decay: 2.697355031967163\n",
      "\tLoss after learning rate decay: 2.204313039779663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [28:11<00:00, 67.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 2.2610063552856445\n",
      "\tValidate loss: 2.6454756259918213\n",
      "\tBest loss 2.4923994541168213 with (DIMENSION, HIDDEN) (12, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dict_hyperparameters = {\n",
    "    \"DIMENSIONS\": [4,6,8,10,12],\n",
    "    \"HIDDENS\": [100, 200, 300, 400, 500]\n",
    "}\n",
    "SIZE_BATCH = 128\n",
    "BEST_PARAMETERS = None\n",
    "BEST_LOSS = float('inf')\n",
    "list_list_parameters = []\n",
    "num_epochs = 10000\n",
    "print(\"HYPERPARAMETER-TRAINING\")\n",
    "for (SIZE_DIMENSION, SIZE_HIDDEN) in tqdm.tqdm(list(itertools.product(*list(dict_hyperparameters.values())))):\n",
    "    C = torch.randn(size=(len(dict_ix_to_token),SIZE_DIMENSION), generator=g, device=device)\n",
    "    W1 = torch.randn(size=(SIZE_CONTEXT-1, SIZE_DIMENSION, SIZE_HIDDEN), generator=g, device=device)\n",
    "    b1 = torch.randn(SIZE_HIDDEN, generator=g, device=device)\n",
    "    W2 = torch.randn(size=(SIZE_HIDDEN, len(dict_ix_to_token)), generator=g, device=device)\n",
    "    b2 = torch.randn(len(dict_ix_to_token), generator=g, device=device)\n",
    "    list_parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "    tag = (SIZE_DIMENSION, SIZE_HIDDEN)\n",
    "    print(f\"TRAINING NEURAL NET with (DIMENSION={SIZE_DIMENSION}, HIDDEN={SIZE_HIDDEN})\")\n",
    "    list_parameters_updated = train_neural_net(Xtr, Ytr, list_parameters, num_epochs, SIZE_BATCH, tag)\n",
    "    loss_train = forward_pass(Xtr, Ytr, list_parameters_updated)\n",
    "    loss_dev = forward_pass(Xdev, Ydev, list_parameters_updated)\n",
    "    print(\"\\tTraining loss:\", loss_train.item())\n",
    "    print(\"\\tValidate loss:\", loss_dev.item())\n",
    "    if loss_dev.item() < BEST_LOSS:\n",
    "        print(f\"\\tBest loss updated for (DIMENSION, HIDDEN): {SIZE_DIMENSION, SIZE_HIDDEN}\")\n",
    "        BEST_LOSS = loss_dev.item()\n",
    "        BEST_PARAMETERS = (SIZE_DIMENSION, SIZE_HIDDEN)\n",
    "\n",
    "print(\"\\tBest loss\",BEST_LOSS, \"with (DIMENSION, HIDDEN)\", BEST_PARAMETERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E02: Initialize Weights More Carefully\n",
    "I was not careful with the intialization of the network in this video. \n",
    "\n",
    "(1) What is the loss you'd get if the predicted probabilities at initialization were perfectly uniform? What loss do we achieve? \n",
    "\n",
    "(2) Can you tune the initialization to get a starting loss that is much more similar to (1)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensors were mistakenly initialized using randn instead of rand, which samples from a standard normal distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E03: Read the Bengio et al 2003 paper (link above), implement and try any idea from the paper. \n",
    "Did it work?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
