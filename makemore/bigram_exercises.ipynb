{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "import tqdm\n",
    "\n",
    "# Env variables\n",
    "DIR_PATH = pathlib.Path(__name__).resolve().parent\n",
    "g = torch.Generator().manual_seed(2147483647) # default seed from the torch docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/c/dev/course-neural-networks-zero-to-hero/makemore')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data to tokens\n",
    "def load_txt_to_list(fname):\n",
    "    path = pathlib.Path(fname)\n",
    "    if not path.exists():\n",
    "        raise f\"{fname} not found!\"\n",
    "\n",
    "    list_out = []\n",
    "    with open(path, \"r\") as file:\n",
    "        list_out = file.read().splitlines()\n",
    "\n",
    "    return list_out\n",
    "\n",
    "def construct_map_token_to_index(list_tokens):\n",
    "    \"\"\"\n",
    "    Returns { token : index }\n",
    "\n",
    "    Map each token in the vocab to a unique integer,\n",
    "    which will be its index into the Bag of words vector\n",
    "\n",
    "    TODO: https://en.wikipedia.org/wiki/Feature_hashing\n",
    "    NOTE: Fatal flaw is set sorting is random, making debugging a little harder\n",
    "    \"\"\"\n",
    "    dict_to_ix = {}\n",
    "    dict_to_word = {}\n",
    "    for i, token in enumerate(set(list_tokens)):\n",
    "        dict_to_ix[token] = i\n",
    "        dict_to_word[i] = token\n",
    "\n",
    "    return dict_to_ix, dict_to_word\n",
    "\n",
    "fname = DIR_PATH / \"names.txt\"\n",
    "words = load_txt_to_list(fname)\n",
    "\n",
    "dict_token_to_ix, dict_ix_to_token = construct_map_token_to_index(\"\".join(words))\n",
    "\n",
    "list_tokens_extra = [\".\"]\n",
    "for token in list_tokens_extra:\n",
    "    dict_token_to_ix[token] = len(dict_token_to_ix)\n",
    "    dict_ix_to_token[len(dict_ix_to_token)] = token\n",
    "\n",
    "list_documents = [ [\".\"] + list(string) + [\".\"] for string in words ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Train a trigram language model\n",
    "\n",
    "Train a trigram language model, i.e., take two characters as an input to predict the 3rd one using counting. Evaluate the loss. Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32033 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32033/32033 [00:00<00:00, 52089.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'e', 'm', 'm'] [26, 11, 0, 0]\n",
      "['e', 'm', 'm', 'a'] [11, 0, 0, 22]\n",
      "['m', 'm', 'a', '.'] [0, 0, 22, 26]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating the ngrams\n",
    "def construct_vector_ngram(tokens_context, dict_index):\n",
    "    return list(map(lambda w: dict_index[w], tokens_context))\n",
    "\n",
    "def construct_n_grams(list_documents, dict_index, size_context):\n",
    "    \"\"\"\n",
    "    Constructs the list ngram indices to be used in other functions\n",
    "    Returns list of list of indices (each list is size size_context)\n",
    "    \"\"\"\n",
    "    if len(min(list_documents, key = len)) < size_context:\n",
    "        raise(\"Smallest token is smaller than context size\")\n",
    "\n",
    "    list_out = []\n",
    "    for tokens in tqdm.tqdm(list_documents):\n",
    "        for j in range(0, len(tokens) - size_context + 1):\n",
    "            vector = construct_vector_ngram(tokens[j:j+size_context], dict_index)\n",
    "            list_out.append(vector)\n",
    "\n",
    "    return list_out\n",
    "\n",
    "SIZE_NGRAMS = 4\n",
    "list_vectors = construct_n_grams(list_documents, dict_token_to_ix, size_context=SIZE_NGRAMS)\n",
    "# Verify the mapping\n",
    "n_bigrams = len(list_documents[0])+1-SIZE_NGRAMS\n",
    "for i in range(n_bigrams):\n",
    "    print(list_documents[0][i:i+SIZE_NGRAMS], list_vectors[i])\n",
    "\n",
    "matrix_ngrams = torch.tensor(list_vectors, dtype = torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting method\n",
    "We'll use the counting method to get the loss based on the MLE, which is the best loss based on my understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def construct_matrix_adjacency_ngram(list_vectors, dict_index, size_context = 2):\n",
    "    \"\"\"\n",
    "    For the 2D case, A[i][j] is vertex i points to vertex j\n",
    "    For the 3D case, A[i1][i2][i3] is vertex i1 and i2 point to i3\n",
    "    Process easily generalizes, but has scaling issues\n",
    "    \"\"\"\n",
    "    out = torch.zeros(tuple([len(dict_index)]*size_context), dtype=torch.int64)\n",
    "    for vector in list_vectors:\n",
    "        out[tuple(vector)] += 1\n",
    "\n",
    "    return out\n",
    "\n",
    "N = construct_matrix_adjacency_ngram(matrix_ngrams, dict_token_to_ix, size_context=SIZE_NGRAMS)\n",
    "\n",
    "size_smoothing = 1 # 0\n",
    "P = (N+size_smoothing) / (N+size_smoothing).sum(dim=SIZE_NGRAMS-1, keepdim=True) # divide by the column-wise sum to normalize N[i,:]\n",
    "# Double-check these all sum to 1\n",
    "matrix_ones = P.sum(dim=-1) \n",
    "torch.allclose(matrix_ones, torch.ones(size = matrix_ones.shape, dtype = matrix_ones.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9636549244271087\n"
     ]
    }
   ],
   "source": [
    "def calculate_log_likelihood(ngrams, P):\n",
    "    \"\"\"\n",
    "    Get the row of each window based on the indices\n",
    "    e.g bigrams would be P[ bigrams[0, :], bigrams[1, :] ]\n",
    "    \"\"\"\n",
    "    probs = P[tuple(ngrams.T)]\n",
    "    log_probs = torch.log(probs)\n",
    "    return -log_probs.sum().item()\n",
    "\n",
    "log_likelihood = calculate_log_likelihood(matrix_ngrams, P) \n",
    "print(log_likelihood / matrix_ngrams.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Select rows of W\n",
    "We saw our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W? Do it for the case of n=2 and n=3 to see a pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([164080, 3, 27])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd70c83d420>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = matrix_ngrams[:,0:SIZE_NGRAMS-1]\n",
    "ys = matrix_ngrams[:,-1] # vector\n",
    "xenc = (\n",
    "    torch.nn.functional.one_hot(xs, num_classes = len(dict_token_to_ix))\n",
    "    .float()\n",
    ")\n",
    "print(xenc.shape)\n",
    "plt.imshow(xenc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True])\n",
      "tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        ...,\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]])\n",
      "tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        ...,\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "# Getting the same result for logits = xenc @ W\n",
    "W = torch.randn(\n",
    "    size = (len(dict_token_to_ix)*(SIZE_NGRAMS-1), len(dict_token_to_ix)), \n",
    "    generator=g,\n",
    "    requires_grad = False\n",
    ")\n",
    "expected = xenc.view(-1, len(dict_token_to_ix)*(SIZE_NGRAMS-1))[0] @ W\n",
    "if SIZE_NGRAMS == 2:\n",
    "    print(expected == W[0*len(dict_ix_to_token)+xs[0]])\n",
    "elif SIZE_NGRAMS == 3:\n",
    "    print(expected == W[0*len(dict_ix_to_token)+xs[0][0],:] + W[1*len(dict_ix_to_token)+xs[0][1],:])\n",
    "elif SIZE_NGRAMS >= 4: \n",
    "    # using python syntax\n",
    "    print(expected == sum(W[i * len(dict_ix_to_token) + xs[0][i], :] for i in range(SIZE_NGRAMS - 1)))\n",
    "    # using torch syntax\n",
    "    print(expected == W[torch.arange(SIZE_NGRAMS-1)*len(dict_ix_to_token) + xs[0][torch.arange(SIZE_NGRAMS-1)]].sum(dim=0))\n",
    "    # using einstein summation notation\n",
    "\n",
    "\n",
    "W_2 = W.reshape((SIZE_NGRAMS-1, len(dict_token_to_ix), len(dict_token_to_ix)))\n",
    "if SIZE_NGRAMS == 2:\n",
    "    print(expected == W_2[0,xs[0][0],:])\n",
    "elif SIZE_NGRAMS == 3:\n",
    "    print(expected == W_2[0,xs[0][0]] + W_2[1,xs[0][1],:])\n",
    "elif SIZE_NGRAMS >= 3: \n",
    "    # using python syntax\n",
    "    print(expected == sum(W_2[i,xs[0][i]] for i in range(SIZE_NGRAMS-1)))\n",
    "    # using torch syntax\n",
    "    print(expected == W_2[torch.arange(SIZE_NGRAMS-1),xs[0]].sum(dim=0))\n",
    "    # using einstein summation notation\n",
    "    print(expected == torch.einsum('jk,jkl -> l', xenc[0], W_2))\n",
    "\n",
    "original_logits = xenc.view(-1, len(dict_token_to_ix)*(SIZE_NGRAMS-1))@W\n",
    "print(original_logits == torch.einsum('ijk,jkl -> il', xenc, W_2))\n",
    "print(original_logits == W_2[torch.arange(SIZE_NGRAMS-1).repeat(xs.shape[0], 1), xs].sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 3.8941891193389893\n",
      "Epoch 10 loss: 3.2265682220458984\n",
      "Epoch 20 loss: 2.956852674484253\n",
      "Epoch 30 loss: 2.8211398124694824\n",
      "Epoch 40 loss: 2.741563558578491\n",
      "Epoch 50 loss: 2.68996262550354\n",
      "Epoch 60 loss: 2.653670072555542\n",
      "Epoch 70 loss: 2.6266300678253174\n",
      "Epoch 80 loss: 2.6056482791900635\n",
      "Epoch 90 loss: 2.588881731033325\n",
      "2.5764482021331787\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "W = torch.randn(\n",
    "    size = ((SIZE_NGRAMS-1), len(dict_token_to_ix), len(dict_token_to_ix)), \n",
    "    generator=g,\n",
    "    requires_grad = True\n",
    ")\n",
    "\n",
    "LEARNING_RATE = 10 # 1 10 50\n",
    "for i in range(100):\n",
    "\n",
    "    # forward pass\n",
    "    logits = W[torch.arange(SIZE_NGRAMS-1).repeat(xs.shape[0], 1), xs].sum(dim=1) # treat these as log-counts\n",
    "    counts = logits.exp() # exp(log-counts) = counts\n",
    "    probs = counts / counts.sum(dim = 1, keepdim=True) # exp(log-counts) / row-sum of exp(log-counts)\n",
    "    # calculate loss (NLL)\n",
    "    loss = -probs[torch.arange(ys.shape[0]),ys].log().mean()\n",
    "\n",
    "    # backward pass\n",
    "    # initialize gradients\n",
    "    W.grad = None # more efficient than setting to 0\n",
    "    loss.backward()\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Epoch {i} loss: {loss.item()}\")\n",
    "\n",
    "    # update\n",
    "    W.data += -LEARNING_RATE * W.grad # going against the gradient reduces the loss\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5:\n",
    "Look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 3.858079433441162\n",
      "Epoch 10 loss: 3.2162160873413086\n",
      "Epoch 20 loss: 2.958385944366455\n",
      "Epoch 30 loss: 2.8256349563598633\n",
      "Epoch 40 loss: 2.7446680068969727\n",
      "Epoch 50 loss: 2.690458297729492\n",
      "Epoch 60 loss: 2.6516213417053223\n",
      "Epoch 70 loss: 2.622840404510498\n",
      "Epoch 80 loss: 2.600919246673584\n",
      "Epoch 90 loss: 2.5836875438690186\n",
      "2.5710151195526123\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "W = torch.randn(\n",
    "    size = ((SIZE_NGRAMS-1), len(dict_token_to_ix), len(dict_token_to_ix)), \n",
    "    generator=g,\n",
    "    requires_grad = True\n",
    ")\n",
    "\n",
    "LEARNING_RATE = 10 # 1 10 50\n",
    "for i in range(100):\n",
    "\n",
    "    # forward pass\n",
    "    logits = W[torch.arange(SIZE_NGRAMS-1).repeat(xs.shape[0], 1), xs].sum(dim=1) # treat these as log-counts\n",
    "    loss = torch.nn.functional.cross_entropy(logits, ys)\n",
    "\n",
    "    # backward pass\n",
    "    # initialize gradients\n",
    "    W.grad = None # more efficient than setting to 0\n",
    "    loss.backward()\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Epoch {i} loss: {loss.item()}\")\n",
    "\n",
    "    # update\n",
    "    W.data += -LEARNING_RATE * W.grad # going against the gradient reduces the loss\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Train the bigram and trigram models on a split dataset\n",
    "\n",
    "Split up the dataset randomly into 80% train set, 10% dev set, 10% test set. \n",
    "Train the bigram and trigram models only on the training set. \n",
    "Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3,4: Tuning the model\n",
    "Use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
